{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Graphml File\n",
    "\n",
    "## Taking a Twitter dataset and creating the graph file which we will insert into graphia for the visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "from emoji import UNICODE_EMOJI\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we load in the dataset and inspect it\n",
    "data = pd.read_csv(\"dataset/gotTwitter.csv\")\n",
    "\n",
    "#Dataset has way too many columns that we don't care about\n",
    "print(data.head())\n",
    "\n",
    "#Lets filter out what we don't need and save again\n",
    "data = data[['created_at', 'screen_name', 'text', 'favorite_count', 'retweet_count', 'is_retweet', 'hashtags', 'mentions_screen_name', 'lang', 'country', 'location', 'followers_count', 'friends_count', 'verified']]\n",
    "\n",
    "data.to_csv(\"dataset/thrones_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now working with the new data since it loads faster\n",
    "data = pd.read_csv(\"dataset/thrones_filtered.csv\")\n",
    "\n",
    "#Can also remove rows without mentions\n",
    "data = data.dropna(subset=['mentions_screen_name'])\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for removing emojis\n",
    "def remove_emojis(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text)\n",
    "\n",
    "#Function for preparing the text for SeaNMF topic modeling\n",
    "def topic_modeling_preprocessing(tweet):\n",
    "\n",
    "    #First remove mentions and hashtags\n",
    "    tweet_list = tweet.split()\n",
    "    tweet_list_filtered = [word for word in tweet_list if word[0]!='@' and word[0]!='#' and \"https\" not in word]\n",
    "\n",
    "    #Remove emojis\n",
    "    filtered_tweet = remove_emojis(\" \".join(tweet_list_filtered))\n",
    "\n",
    "    #first remove punctuation\n",
    "    tweet_no_punct = filtered_tweet.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "    #Now can lemmatize and POS tag the words and remove closed class words\n",
    "    tweet_pos = nltk.pos_tag(tweet_no_punct.split())\n",
    "    \n",
    "    #define list of closed classes\n",
    "    #Keep digits for now as would be nice to see season progression details\n",
    "    closed_classes = ['CC', 'DT', 'IN', 'PRP', 'PRP$', 'WDT', 'WP', 'WP$', \"LS\", \"MD\", \"PDT\"]\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "    open_class_tweet = [pos_tuple[0] for pos_tuple in tweet_pos if pos_tuple[1] not in closed_classes and pos_tuple[0] not in stopword_list]\n",
    "\n",
    "    #now final step to lemmatize all the words in this list\n",
    "    #initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tweet = [lemmatizer.lemmatize(word) for word in open_class_tweet]\n",
    "\n",
    "    return \" \".join(lemmatized_tweet)\n",
    "\n",
    "\n",
    "#Lets set up some empty lists where we will store the extracted data which will then be made in to the DF\n",
    "tweets = []\n",
    "sources = []\n",
    "targets = []\n",
    "favorite_counts = []\n",
    "retweet_count = []\n",
    "hashtags = []\n",
    "lang = []\n",
    "location = []\n",
    "followers_count = []\n",
    "friends_count = []\n",
    "verified = []\n",
    "sentiment_scores = []\n",
    "text_for_topic_modeling = []\n",
    "\n",
    "#Initialize VADER sentiment score analyzer\n",
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Iterate over our original dataframe\n",
    "for index in range(len(data)):\n",
    "\n",
    "    #Get the mentions as a list for each tweet\n",
    "    mentions = data['mentions_screen_name'][index]\n",
    "    mentions_list = mentions.split()\n",
    "\n",
    "    #Now we neet to iterate over this list and add information to lists accordingly\n",
    "    #Let's also add the VADER sentiment score\n",
    "    for mention in mentions_list:\n",
    "        tweets.append(data['text'][index])\n",
    "        sources.append(data['screen_name'][index])\n",
    "        targets.append(mention)\n",
    "        favorite_counts.append(data['favorite_count'][index])\n",
    "        retweet_count.append(data['retweet_count'][index])\n",
    "        hashtags.append(data['hashtags'][index])\n",
    "        lang.append(data['lang'][index])\n",
    "        location.append(data['location'][index])\n",
    "        followers_count.append(data['followers_count'][index])\n",
    "        friends_count.append(data['friends_count'][index])\n",
    "        verified.append(data['verified'][index])\n",
    "        sentiment_scores.append(sid_obj.polarity_scores(data['text'][index])['compound'])\n",
    "        text_for_topic_modeling.append(topic_modeling_preprocessing(data['text'][index]))\n",
    "\n",
    "#Make final mentions DF from which we will get graphml file\n",
    "mentions_df = pd.DataFrame({\n",
    "        'tweets' : tweets,\n",
    "        'sources' : sources,\n",
    "        'targets' : targets,\n",
    "        'favorite_counts' : favorite_counts,\n",
    "        'hashtags' : hashtags,\n",
    "        'lang' : lang,\n",
    "        'location':location,\n",
    "        'followers_count':followers_count,\n",
    "        'friends_count':friends_count,\n",
    "        'verified':verified,\n",
    "        'sentiment_scores': sentiment_scores,\n",
    "        'topic_modeling_text' : text_for_topic_modeling,\n",
    "    })\n",
    "\n",
    "mentions_df.to_csv(\"dataset/mentions.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets run some topic modeling too so we can attach topic labels to our graph\n",
    "mentions_df = pd.read_csv(\"dataset/mentions.csv\")\n",
    "\n",
    "#Now need to make the text file for SeaNMF\n",
    "#Because you have changed the data and expanded on mentions, lots of text duplicates to get rid of\n",
    "text_list = set(mentions_df['topic_modeling_text'].to_list())\n",
    "\n",
    "with open(\"topic_modeling.txt\", 'w') as f:\n",
    "\n",
    "    for tweet in text_list:\n",
    "        if not isinstance(tweet, float):\n",
    "            f.write(tweet + \"\\n\")\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now need to assign the topic number and at same time topic label to our data\n",
    "#Load in some of the SeaNMF matrices and structures\n",
    "\n",
    "#Function for loading in the vocab (from SeaNMF)\n",
    "def read_vocab(file_name):\n",
    "    print('read vocabulary')\n",
    "    print('-'*50)\n",
    "    vocab = []\n",
    "    fp = open(file_name, 'r')\n",
    "    for line in fp:\n",
    "        arr = re.split('\\s', line[:-1])\n",
    "        vocab.append(arr[0])\n",
    "    fp.close()\n",
    "\n",
    "    return vocab\n",
    "\n",
    "#load in the vocab file\n",
    "vocab = read_vocab('SeaNMF/data/vocab.txt')\n",
    "\n",
    "#Load in the model results (W Matrix)\n",
    "W = np.loadtxt('SeaNMF/seanmf_results/60/W.txt', dtype=float)\n",
    "\n",
    "#Load in the topics and the topic names and make the mapping dict\n",
    "topic_names = pd.read_csv(\"dataset/topic_word_lists.csv\")\n",
    "map_topics = topic_names[[\"Topic Name\", \"Topic Number\"]]\n",
    "\n",
    "mapping_dict = dict(zip(map_topics['Topic Number'], map_topics['Topic Name']))\n",
    "\n",
    "#Function to get the most likely topic for a given tweet, using the resulting matrices and vocab file\n",
    "def get_most_likely_topic(tweet, vocab, W, mapping_dict):\n",
    "\n",
    "    #Tokenize the string\n",
    "    if not isinstance(tweet, float):\n",
    "        tokenized_string = tweet.split()\n",
    "\n",
    "        #Get the indexes from vocab list for the new string\n",
    "        vocab_indexes = []\n",
    "        for token in tokenized_string:\n",
    "            if token in vocab:\n",
    "                vocab_indexes.append(vocab.index(token))\n",
    "\n",
    "        #Using the indexes, subset W so it just contains all those rows\n",
    "        subset_W = W[vocab_indexes, :]\n",
    "        \n",
    "\n",
    "        #Get a sum of the columns (divide by length of columns) to normalize or something but not sure about this yet\n",
    "        sums_of_columns = []\n",
    "        if subset_W.size != 0:\n",
    "            for column in subset_W.T:\n",
    "                sums_of_columns.append(sum(column)/subset_W.shape[1])\n",
    "        else:\n",
    "            return \"not found\"\n",
    "        #Get the indexes of top 10 topics of the document\n",
    "        top_topics = sorted(range(len(sums_of_columns)), key=lambda i: sums_of_columns[i])[-15:]\n",
    "        top_topics.reverse()\n",
    "\n",
    "\n",
    "        #Filter out these topics (MISC, TV Service)\n",
    "        list_filter_topics = [4, 3, 5, 2, 1, 38, 45, 26, 25, 29, 11]\n",
    "        \n",
    "        #Set up the top topic\n",
    "        i = 0\n",
    "        #Check if top topic is in list, if it is, increment and take the next one\n",
    "        while (top_topics[i] + 1) in list_filter_topics:\n",
    "            i = i + 1\n",
    "\n",
    "        #Return the value at the dict key\n",
    "        return mapping_dict[top_topics[i]+1]\n",
    "    else:\n",
    "        return \"not found\"\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "#Apply to the topic modelling text for each mention pair\n",
    "mentions_df['Highest_topic'] = mentions_df.topic_modeling_text.progress_apply(lambda x: get_most_likely_topic(x, vocab, W, mapping_dict))\n",
    "mentions_df.to_csv(\"dataset/final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's make the graph\n",
    "def make_graph_file(mentions_data):\n",
    "\n",
    "    #initialize the graph\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    #Make the graph, this is where later we will add the attributes too\n",
    "    for index, relation in mentions_data.iterrows():\n",
    "        graph.add_edge(relation[\"sources\"], relation[\"targets\"])\n",
    "        graph.add_node(relation[\"sources\"], topic = relation['Highest_topic'], polarity = relation['sentiment_scores'], followers = relation[\"followers_count\"], friends = relation[\"friends_count\"], likes = relation[\"favorite_counts\"])\n",
    "        graph.add_node(relation[\"targets\"])\n",
    "    \n",
    "    #Now lets get the largest component subgraph\n",
    "    components = sorted(nx.connected_components(graph), key = len, reverse = True)\n",
    "    largest_component = graph.subgraph(components[0])\n",
    "\n",
    "    #Save the graphml file\n",
    "    nx.write_graphml(largest_component, \"dataset/final_graph.graphml\")\n",
    "\n",
    "\n",
    "make_graph_file(mentions_df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
